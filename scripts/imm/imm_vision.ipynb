{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwO2gAgiJS2",
        "outputId": "73a1bc98-540b-4fbc-a0a1-cb9282304b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (0.21.0)\n",
            "Requirement already satisfied: diffusers in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (0.33.1)\n",
            "Requirement already satisfied: scikit-image in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (0.25.2)\n",
            "Requirement already satisfied: scikit-video in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (1.1.11)\n",
            "Requirement already satisfied: zarr in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (2.18.3)\n",
            "Requirement already satisfied: numcodecs in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (0.13.1)\n",
            "Requirement already satisfied: pygame in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (2.6.1)\n",
            "Requirement already satisfied: pymunk in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (6.11.1)\n",
            "Requirement already satisfied: gym in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (0.26.2)\n",
            "Requirement already satisfied: shapely in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (2.1.0)\n",
            "Requirement already satisfied: opencv-python in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (4.11.0.86)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torchvision) (2.2.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: importlib-metadata in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from diffusers) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: asciitree in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from zarr) (0.3.3)\n",
            "Requirement already satisfied: fasteners in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from zarr) (0.19)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from pymunk) (1.17.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from gym) (0.0.8)\n",
            "Collecting beautifulsoup4 (from gdown)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: pycparser in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.2)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from requests->diffusers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from requests->diffusers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ea/diffusion1/.venv/lib/python3.10/site-packages (from requests->diffusers) (2025.1.31)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
            "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 gdown-5.2.0 soupsieve-2.6\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!pip3 install torch torchvision diffusers \\\n",
        "scikit-image scikit-video zarr numcodecs \\\n",
        "pygame pymunk gym shapely opencv-python gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a8e97e26d65147efa2ddb176214e818a",
            "19e913a07b044b2d91ca7b5d2dbbe8f7",
            "1de97479fc3e44caae336cf0b1082d02",
            "b8d4e152c232416f82f5fb30e2478ef8",
            "d66b3f8971b24c81abaefe111e6383a6",
            "11d14b9bf0f74bf7bd1b58e6a26fd49e",
            "e9fde27bd58b499bb386b36314d9efca",
            "5407db80343c44c79350021cd98204f8",
            "68e9f5994766480dac5dc77ce043a2e9",
            "d00c195ac5644bd0aba81dccf56e2df0",
            "54c35b52c4e24543872c8335c935c410"
          ]
        },
        "id": "VrX4VTl5pYNq",
        "outputId": "63f38652-f298-4723-bd5c-e1bbe5f9d961"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ea/diffusion1/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "L5E-nR6ornyg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "#@markdown And it's subclass `PushTImageEnv`.\n",
        "#@markdown\n",
        "#@markdown **Goal**: push the gray T-block into the green area.\n",
        "#@markdown\n",
        "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "\n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "\n",
        "    def reset(self):\n",
        "        seed = self._seed\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "\n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for i in range(n_steps):\n",
        "                # Step PD control.\n",
        "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Step physics.\n",
        "                self.space.step(dt)\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "        terminated = done\n",
        "        truncated = done\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "\n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "\n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2],\n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "\n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # Add collision handeling\n",
        "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
        "        self.collision_handeler.post_solve = self._handle_collision\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body\n",
        "\n",
        "\n",
        "class PushTImageEnv(PushTEnv):\n",
        "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None,\n",
        "            damping=None,\n",
        "            render_size=96):\n",
        "        super().__init__(\n",
        "            legacy=legacy,\n",
        "            block_cog=block_cog,\n",
        "            damping=damping,\n",
        "            render_size=render_size,\n",
        "            render_action=False)\n",
        "        ws = self.window_size\n",
        "        self.observation_space = spaces.Dict({\n",
        "            'image': spaces.Box(\n",
        "                low=0,\n",
        "                high=1,\n",
        "                shape=(3,render_size,render_size),\n",
        "                dtype=np.float32\n",
        "            ),\n",
        "            'agent_pos': spaces.Box(\n",
        "                low=0,\n",
        "                high=ws,\n",
        "                shape=(2,),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "        })\n",
        "        self.render_cache = None\n",
        "\n",
        "    def _get_obs(self):\n",
        "        img = super()._render_frame(mode='rgb_array')\n",
        "\n",
        "        agent_pos = np.array(self.agent.position)\n",
        "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
        "        obs = {\n",
        "            'image': img_obs,\n",
        "            'agent_pos': agent_pos\n",
        "        }\n",
        "\n",
        "        # draw action\n",
        "        if self.latest_action is not None:\n",
        "            action = np.array(self.latest_action)\n",
        "            coord = (action / 512 * 96).astype(np.int32)\n",
        "            marker_size = int(8/96*self.render_size)\n",
        "            thickness = int(1/96*self.render_size)\n",
        "            cv2.drawMarker(img, coord,\n",
        "                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                markerSize=marker_size, thickness=thickness)\n",
        "        self.render_cache = img\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode):\n",
        "        assert mode == 'rgb_array'\n",
        "\n",
        "        if self.render_cache is None:\n",
        "            self._get_obs()\n",
        "\n",
        "        return self.render_cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknH8Qfqrtc9",
        "outputId": "c97c843e-0b23-4c0a-c123-cd72c407e163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obs['image'].shape: (3, 96, 96) float32, [0,1]\n",
            "obs['agent_pos'].shape: (2,) float32, [0,512]\n",
            "action.shape:  (2,) float32, [0,512]\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "env = PushTImageEnv()\n",
        "\n",
        "# 1. seed env for initial state.\n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "env.seed(1000)\n",
        "\n",
        "# 2. must reset before use\n",
        "obs, info = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
        "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
        "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "vHepJOFBucwg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTImageDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data ((image, agent_pos), action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
        "#@markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n",
        "#@markdown  - key `action`: shape (pred_horizon, 2)\n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int,\n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "\n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx,\n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx,\n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0),\n",
        "        'std': np.std(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dataset_path: str,\n",
        "                 pred_horizon: int,\n",
        "                 obs_horizon: int,\n",
        "                 action_horizon: int):\n",
        "\n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "\n",
        "        # float32, [0,1], (N,96,96,3)\n",
        "        train_image_data = dataset_root['data']['img'][:]\n",
        "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
        "        # (N,3,96,96)\n",
        "\n",
        "        # (N, D)\n",
        "        train_data = {\n",
        "            # first two dims of state vector are agent (i.e. gripper) locations\n",
        "            'agent_pos': dataset_root['data']['state'][:,:2],\n",
        "            'action': dataset_root['data']['action'][:]\n",
        "        }\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "\n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "            stats[key+'_normalized'] = get_data_stats(normalized_train_data[key])\n",
        "\n",
        "        # images are already normalized\n",
        "        normalized_train_data['image'] = train_image_data\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
        "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZiHF3lzvB6k",
        "outputId": "e6256ff1-ab20-41d4-a9ea-7c035196bf3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['image'].shape: torch.Size([1024, 2, 3, 96, 96])\n",
            "batch['agent_pos'].shape: torch.Size([1024, 2, 2])\n",
            "batch['action'].shape torch.Size([1024, 16, 2])\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "dataset = PushTImageDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1024,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['image'].shape:\", batch['image'].shape)\n",
        "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24208\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float32(0.40121755)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(dataset))\n",
        "stats['action_normalized']['std'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "cellView": "form",
        "id": "X-XRB_g3vsgf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class IMMloss(nn.Module):\n",
        "    \"\"\"\n",
        "    IMM loss function using the Laplace kernel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_horizon, pred_horizon, num_particles):\n",
        "        super(IMMloss, self).__init__()\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.num_particles = num_particles\n",
        "        \n",
        "    def laplace_kernel(self, x, y, w_scale, eps=0.006, dim_normalize=True):\n",
        "        \"\"\"\n",
        "        Laplace kernel: exp(w_scale * max(||x-y||_2, eps)/D)\n",
        "        \n",
        "        Args:\n",
        "            x, y: input tensors\n",
        "            w_scale: scaling factor (time-dependent)\n",
        "            eps: small constant to avoid undefined gradients\n",
        "            dim_normalize: whether to normalize by dimensionality D\n",
        "        \"\"\"\n",
        "        D = x.shape[-1] if dim_normalize else 1.0\n",
        "        distance = torch.norm(x - y, p=2, dim=-1)\n",
        "        # Apply max to avoid zero gradients\n",
        "        distance = torch.clamp(distance, min=eps)\n",
        "        return torch.exp(-w_scale * distance / D)\n",
        "    \n",
        "    def forward(self, model_outputs, time_weights, stop_gradient_outputs):\n",
        "        \"\"\"\n",
        "        Compute the IMM loss for a batch of model outputs.\n",
        "        \n",
        "        Args:\n",
        "            model_outputs: Dictionary containing:\n",
        "                - ys_t: outputs from time t to s [B, self.pred_horizon, self.obs_horizon]\n",
        "                - ys_r: outputs from time r to s [B, self.pred_horizon, self.obs_horizon]\n",
        "                - w_scale: time-dependent scaling factors [B]\n",
        "            time_weights: w(s,t) weights [B/M]\n",
        "            stop_gradient_outputs: Optional dictionary with same structure as model_outputs\n",
        "                                   containing the detached outputs (θ-)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Extract batch size and reshape for group processing\n",
        "        batch_size = model_outputs['ys_t'].shape[0]\n",
        "        M = self.num_particles\n",
        "        num_groups = batch_size // M\n",
        "        \n",
        "        # Flatten pred_horizon and obs_horizon dimensions before reshaping\n",
        "        # Reshape tensors to [num_groups, M, D]\n",
        "        ys_t = model_outputs['ys_t'].reshape(batch_size, -1).reshape(num_groups, M, -1)\n",
        "        ys_r_stop = stop_gradient_outputs['ys_r'].reshape(batch_size, -1).reshape(num_groups, M, -1)\n",
        "        w_scale = model_outputs['w_scale'].reshape(num_groups, M)\n",
        "        \n",
        "\n",
        "        \n",
        "        # Reshape time weights to [num_groups] by extracting the first element of each group\n",
        "        time_weights = time_weights.reshape(num_groups, M)[:,0].reshape(-1)\n",
        "        \n",
        "        total_loss = 0.0\n",
        "        for i in range(num_groups):\n",
        "            group_loss = 0.0\n",
        "            \n",
        "            # Compute the kernel matrices\n",
        "            for j in range(M):\n",
        "                for k in range(M):\n",
        "                    # First term: k(f_s,t^θ(x_t^(i,j)), f_s,t^θ(x_t^(i,k)))\n",
        "                    term1 = self.laplace_kernel(ys_t[i, j], ys_t[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Second term: k(f_s,r^θ-(x_r^(i,j)), f_s,r^θ-(x_r^(i,k)))\n",
        "                    term2 = self.laplace_kernel(ys_r_stop[i, j], ys_r_stop[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Third term: -2k(f_s,t^θ(x_t^(i,j)), f_s,r^θ-(x_r^(i,k)))\n",
        "                    term3 = -2.0 * self.laplace_kernel(ys_t[i, j], ys_r_stop[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Sum up the terms\n",
        "                    group_loss += term1 + term2 + term3\n",
        "            \n",
        "            # Apply time-dependent weighting\n",
        "            group_loss = group_loss * time_weights[i] / (M * M)\n",
        "            total_loss += group_loss\n",
        "        \n",
        "        # Average over the number of groups\n",
        "        return total_loss / num_groups\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        \n",
        "        # Second encoder for timestep s (for IMM)\n",
        "        diffusion_step_encoder_s = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "            \n",
        "        # Total conditioning dimensions: t embedding + s embedding + global conditioning\n",
        "        cond_dim = dsed * 2 + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.diffusion_step_encoder_s = diffusion_step_encoder_s\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float],\n",
        "            timestep_s: Union[torch.Tensor, float],\n",
        "            global_cond):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,), diffusion step\n",
        "        timestep_s: (B,), diffusion step s (for IMM)\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        # timesteps = timestep\n",
        "        # if not torch.is_tensor(timesteps):\n",
        "        #     # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "        #     timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        # elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "        #     timesteps = timesteps[None].to(sample.device)\n",
        "        # # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        # timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        t_emb = self.diffusion_step_encoder(timestep)\n",
        "        s_emb = self.diffusion_step_encoder_s(timestep_s)\n",
        "\n",
        "        global_feature = torch.cat([t_emb, s_emb], dim=-1)\n",
        "\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n",
        "    \n",
        "class RoboIMM:\n",
        "    \"\"\"\n",
        "    Simplified class for IMM with the robotics UNet.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        nets,\n",
        "        sigma_data,\n",
        "        obs_horizon,\n",
        "        pred_horizon,\n",
        "        num_particles\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the IMM sampler.\n",
        "        \"\"\"\n",
        "        self.nets = nets\n",
        "        self.sigma_data = sigma_data\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.num_particles = num_particles\n",
        "\n",
        "        self.loss = IMMloss(obs_horizon=obs_horizon, pred_horizon=pred_horizon, num_particles=num_particles)\n",
        "\n",
        "    def get_alpha_sigma(self, t):\n",
        "        \"\"\"Get alpha and sigma values for time t.\"\"\"\n",
        "        # Using the \"flow matching\" schedule\n",
        "        alpha_t = (1 - t)\n",
        "        sigma_t = t\n",
        "        return alpha_t, sigma_t\n",
        "    \n",
        "    def ddim(self, yt, y, s, t):\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t)\n",
        "        alpha_s, sigma_s = self.get_alpha_sigma(s)\n",
        "\n",
        "        alpha_s = alpha_s.reshape(-1,1,1)\n",
        "        sigma_s = sigma_s.reshape(-1,1,1)\n",
        "        alpha_t = alpha_t.reshape(-1,1,1)\n",
        "        sigma_t = sigma_t.reshape(-1,1,1)\n",
        "        \n",
        "        ys = (alpha_s -   alpha_t * sigma_s / sigma_t) * y + sigma_s / sigma_t * yt\n",
        "        return ys\n",
        "    \n",
        "    def sample(self, shape, image, agent_pos, steps=20, sampling_method=\"ddim\"):\n",
        "        \"\"\"\n",
        "        Generate samples using IMM sampling.\n",
        "        \n",
        "        Args:\n",
        "            shape: Shape of the samples to generate\n",
        "            steps: Number of sampling steps\n",
        "            global_cond: Global conditioning\n",
        "            sampling_method: \"ddim\"\n",
        "            \n",
        "        Returns:\n",
        "            Generated samples\n",
        "        \"\"\"\n",
        "        device = next(self.nets.parameters()).device\n",
        "        image = image.to(device)\n",
        "        agent_pos = agent_pos.to(device)\n",
        "        \n",
        "        # vision encoder\n",
        "        image_features = self.nets['vision_encoder'](image.flatten(end_dim=1))\n",
        "        \n",
        "        # (2,512)\n",
        "        image_features = image_features.reshape(*image.shape[:2],-1)\n",
        "        \n",
        "        # (1,2,512)\n",
        "        \n",
        "        obs = torch.cat([image_features, agent_pos],dim=-1)\n",
        "        # (1,2,514)\n",
        "\n",
        "        x = torch.randn(shape, device=device) * self.sigma_data\n",
        "        \n",
        "        # Define time steps (uniform steps from 1 to 0)\n",
        "        times = torch.linspace(0.994, 0.006, steps + 1, device=device)\n",
        "                \n",
        "        for i in range(steps):\n",
        "            t = times[i]\n",
        "            s = times[i + 1]\n",
        "            \n",
        "            # Create batched time tensors\n",
        "            t_batch = torch.full((shape[0],), t, device=device)\n",
        "            s_batch = torch.full((shape[0],), s, device=device)\n",
        "            \n",
        "            # Run model forward\n",
        "            with torch.no_grad():\n",
        "                x = self.predict(x, t_batch, s_batch, obs.flatten(start_dim=1))\n",
        "                # noise = nets['noise_pred_net'](\n",
        "                #     sample=noised_action,\n",
        "                #     timestep=diffusion_iter,\n",
        "                #     timestep_s=diffusion_iter,\n",
        "                #     global_cond=)\n",
        "            \n",
        "            # Apply sampling function based on method\n",
        "            # if sampling_method == \"ddim\":\n",
        "            #     x = self.ddim(x, pred, s_batch.view(-1, 1, 1), t_batch.view(-1, 1, 1))\n",
        "            # else:\n",
        "            #     raise ValueError(f\"Unknown sampling method: {sampling_method}\")\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def calculate_weights(self, s_times, t_times):\n",
        "        \"\"\"\n",
        "        Calculate the time-dependent weighting function w(s,t)\n",
        "        \"\"\"\n",
        "        b = 5  # Hyperparameter from paper\n",
        "        a = 1    # Hyperparameter from paper (a ∈ {1, 2})\n",
        "\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t_times)\n",
        "        \n",
        "        # Calculate log-SNR values\n",
        "        log_snr_t = 2 * torch.log(alpha_t / sigma_t)\n",
        "        dlog_snr_t = 2 / (torch.square(t_times) - t_times)\n",
        "        \n",
        "        # Calculate coefficient based on equation 13\n",
        "        sigmoid_term = torch.sigmoid(b - log_snr_t)\n",
        "        \n",
        "        snr_term = (alpha_t ** a) / (alpha_t ** 2 + sigma_t ** 2)\n",
        "        \n",
        "        return 0.5 * sigmoid_term * -1.0 * dlog_snr_t * snr_term\n",
        "    \n",
        "    def predict(self, xt, t, s, obs_cond):\n",
        "        pass\n",
        "        c = 1000.0\n",
        "        cskip = 1.0\n",
        "        cout = -(t-s) * self.sigma_data\n",
        "        c_timestep = c * t\n",
        "        c_timestep_s = c * s\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t)\n",
        "        c_in = (torch.pow(alpha_t, 2) + torch.pow(sigma_t, 2)).rsqrt() / self.sigma_data\n",
        "        xs = self.nets['noise_net'](xt*c_in.reshape(-1,1,1), c_timestep, c_timestep_s, obs_cond)\n",
        "        return cskip * xt + cout.reshape(-1,1,1) * xs\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=100, lr=1e-4, device='cuda'):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        \"\"\"\n",
        "        # Standard ADAM optimizer\n",
        "        # Note that EMA parametesr are not optimized\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            params=self.nets.parameters(),\n",
        "            lr=lr, weight_decay=0)\n",
        "\n",
        "        # Cosine LR schedule\n",
        "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "\n",
        "        self.nets.train()\n",
        "        \n",
        "        with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "            # epoch loop\n",
        "            for epoch_idx in tglobal:\n",
        "                # batch loop\n",
        "                with tqdm(train_loader, desc='Batch', leave=False) as tepoch:\n",
        "                    for batch_idx, nbatch in enumerate(tepoch):\n",
        "                        nimage = nbatch['image'][:,:self.obs_horizon].to(device)\n",
        "                        nagent_pos = nbatch['agent_pos'][:,:self.obs_horizon].to(device)\n",
        "                        naction = nbatch['action'].to(device)\n",
        "                        B = nagent_pos.shape[0]\n",
        "\n",
        "                        # get vision features\n",
        "                        image_features = self.nets['vision_encoder'](\n",
        "                        nimage.flatten(end_dim=1))\n",
        "                        image_features = image_features.reshape(*nimage.shape[:2],-1)\n",
        "\n",
        "                        # get times\n",
        "                        num_groups = B // self.num_particles\n",
        "                        s_times = torch.rand(num_groups, device=device)\n",
        "                        t_times = s_times + (1 - s_times) * torch.rand(num_groups, device=device)\n",
        "                        r_times = s_times + (t_times - s_times) * torch.rand(num_groups, device=device)\n",
        "\n",
        "                        # times need to be shape (B,), currently they are shape (num_groups,)\n",
        "                        s_times = s_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "                        t_times = t_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "                        r_times = r_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "\n",
        "                        noise = torch.randn_like(naction) * self.sigma_data\n",
        "                        \n",
        "                        x_t = self.ddim(yt=noise, y=naction, s=t_times, t=torch.ones_like(t_times))\n",
        "                        x_r = self.ddim(yt=x_t, y=naction, s=r_times, t=t_times)\n",
        "                        \n",
        "\n",
        "                        obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
        "                        obs_cond = obs_features.flatten(start_dim=1)\n",
        "                        \n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        pred_grad = self.predict(x_t, t_times, s_times, obs_cond)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            pred_nograd = self.predict(x_r, r_times, s_times, obs_cond)\n",
        "    \n",
        "                        time_weights = self.calculate_weights(s_times, t_times)\n",
        "                        \n",
        "                        # Reshape predictions to match expected dimensions\n",
        "                        # Assuming pred_grad and pred_nograd are [B, sequence_length, action_dim]\n",
        "                        # Reshape to [B, self.pred_horizon, self.obs_horizon]\n",
        "                        model_outputs = {\n",
        "                            'ys_t': pred_grad.reshape(B, self.pred_horizon, self.obs_horizon),\n",
        "                            'w_scale': 1.0 / torch.abs((t_times - s_times) * self.sigma_data)\n",
        "                        }\n",
        "                        \n",
        "                        stop_gradient_outputs = {\n",
        "                            'ys_r': pred_nograd.reshape(B, self.pred_horizon, self.obs_horizon).detach()\n",
        "                        }\n",
        "                        \n",
        "                        loss = self.loss(model_outputs, time_weights, stop_gradient_outputs)\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        lr_scheduler.step()\n",
        "\n",
        "                        if batch_idx % 10 == 0:\n",
        "                            print(f\"Epoch {epoch_idx}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "                        if batch_idx == 0 and epoch_idx % 10 == 0:\n",
        "                            # save model checkpoint\n",
        "                            torch.save({\n",
        "                                'epoch': epoch_idx,\n",
        "                                'model_state_dict': self.nets.state_dict(),\n",
        "                                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                                'loss': loss.item()\n",
        "                            }, f\"ckpts/model_checkpoint_{epoch_idx}_{batch_idx}_{loss.item()}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yXq4r744aMh1"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Vision Encoder**\n",
        "#@markdown\n",
        "#@markdown Defines helper functions:\n",
        "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
        "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
        "\n",
        "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
        "    \"\"\"\n",
        "    name: resnet18, resnet34, resnet50\n",
        "    weights: \"IMAGENET1K_V1\", None\n",
        "    \"\"\"\n",
        "    # Use standard ResNet implementation from torchvision\n",
        "    func = getattr(torchvision.models, name)\n",
        "    resnet = func(weights=weights, **kwargs)\n",
        "\n",
        "    # remove the final fully connected layer\n",
        "    # for resnet18, the output dim should be 512\n",
        "    resnet.fc = torch.nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "\n",
        "def replace_submodules(\n",
        "        root_module: nn.Module,\n",
        "        predicate: Callable[[nn.Module], bool],\n",
        "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Replace all submodules selected by the predicate with\n",
        "    the output of func.\n",
        "\n",
        "    predicate: Return true if the module is to be replaced.\n",
        "    func: Return new module to use.\n",
        "    \"\"\"\n",
        "    if predicate(root_module):\n",
        "        return func(root_module)\n",
        "\n",
        "    bn_list = [k.split('.') for k, m\n",
        "        in root_module.named_modules(remove_duplicate=True)\n",
        "        if predicate(m)]\n",
        "    for *parent, k in bn_list:\n",
        "        parent_module = root_module\n",
        "        if len(parent) > 0:\n",
        "            parent_module = root_module.get_submodule('.'.join(parent))\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            src_module = parent_module[int(k)]\n",
        "        else:\n",
        "            src_module = getattr(parent_module, k)\n",
        "        tgt_module = func(src_module)\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            parent_module[int(k)] = tgt_module\n",
        "        else:\n",
        "            setattr(parent_module, k, tgt_module)\n",
        "    # verify that all modules are replaced\n",
        "    bn_list = [k.split('.') for k, m\n",
        "        in root_module.named_modules(remove_duplicate=True)\n",
        "        if predicate(m)]\n",
        "    assert len(bn_list) == 0\n",
        "    return root_module\n",
        "\n",
        "def replace_bn_with_gn(\n",
        "    root_module: nn.Module,\n",
        "    features_per_group: int=16) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Relace all BatchNorm layers with GroupNorm.\n",
        "    \"\"\"\n",
        "    replace_submodules(\n",
        "        root_module=root_module,\n",
        "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
        "        func=lambda x: nn.GroupNorm(\n",
        "            num_groups=x.num_features//features_per_group,\n",
        "            num_channels=x.num_features)\n",
        "    )\n",
        "    return root_module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "e362d582-ceac-4cdd-e866-c34858593696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 8.414285e+07\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# construct ResNet18 encoder\n",
        "# if you have multiple camera views, use seperate encoder weights for each view.\n",
        "vision_encoder = get_resnet('resnet18')\n",
        "\n",
        "# IMPORTANT!\n",
        "# replace all BatchNorm with GroupNorm to work with EMA\n",
        "# performance will tank if you forget to do this!\n",
        "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
        "\n",
        "# ResNet18 has output dim of 512\n",
        "vision_feature_dim = 512\n",
        "# agent_pos is 2 dimensional\n",
        "lowdim_obs_dim = 2\n",
        "# observation feature has 514 dims in total per step\n",
        "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# the final arch has 2 parts\n",
        "nets = nn.ModuleDict({\n",
        "    'vision_encoder': vision_encoder,\n",
        "    'noise_net': noise_pred_net\n",
        "})\n",
        "\n",
        "imm = RoboIMM(\n",
        "    nets=nets,\n",
        "    sigma_data=stats['action_normalized']['std'].mean(),\n",
        "    obs_horizon=obs_horizon,\n",
        "    pred_horizon=pred_horizon,\n",
        "    num_particles=4\n",
        ")\n",
        "\n",
        "# demo\n",
        "with torch.no_grad():\n",
        "    # example inputs\n",
        "    image = torch.zeros((1, obs_horizon,3,96,96))\n",
        "    agent_pos = torch.zeros((1, obs_horizon, 2))\n",
        "    # vision encoder\n",
        "    image_features = nets['vision_encoder'](\n",
        "        image.flatten(end_dim=1))\n",
        "    # (2,512)\n",
        "    image_features = image_features.reshape(*image.shape[:2],-1)\n",
        "    # (1,2,512)\n",
        "    obs = torch.cat([image_features, agent_pos],dim=-1)\n",
        "    # (1,2,514)\n",
        "\n",
        "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "    diffusion_iter = 1.0 * torch.ones((1,))\n",
        "    diffusion_iter_s = 0.0 * torch.ones((1,))\n",
        "\n",
        "    imm.sample(noised_action.shape, image=image, agent_pos=agent_pos, steps=10, sampling_method=\"ddim\")\n",
        "    # # the noise prediction network\n",
        "    # # takes noisy action, diffusion iteration and observation as input\n",
        "    # # predicts the noise added to action\n",
        "    # noise = nets['noise_net'](\n",
        "    #     sample=noised_action,\n",
        "    #     timestep=diffusion_iter,\n",
        "    #     timestep_s=diffusion_iter,\n",
        "    #     global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "    # # illustration of removing noise\n",
        "    # # the actual noise removal is performed by NoiseScheduler\n",
        "    # # and is dependent on the diffusion noise schedule\n",
        "    # denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = nets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b34bf5a5d603446fb453bbed31c4469c",
            "0873a2dee0e44b0fb3e8445d94c27171",
            "410476fa4ac14dd2b78377f4d779402f",
            "09152dbe3c8543aa809aa592afdc53f1",
            "e2a91fa6d1514913892c0def2e2ecf78",
            "7fd765bcd2a34db49a02839ba1c4f518",
            "d9054c34284045eda3546a21bb5fafc3",
            "a92fc95c6a3d496997da90c87e24ba01",
            "9a98272475d940b1b98438c4d02dcc05",
            "47b6383ed3b64b7e8c7e66d0e7a468d9",
            "1cdde5afac8041bf9b75e0e80bd80630",
            "70b4b657d68648d9be98c099d061cd5a",
            "8c0237e38cfd4143aa423dea26637965",
            "44c486fa8c4241f4a1a245f4e24da769",
            "12ef3c026774405cb91faf18d7fd8afa",
            "99c0577d549c4dd4a61d9ec0cf6254d5",
            "5124950790874225b5cca7556f122f9a",
            "f609505b750747dcaff8a950131743e9",
            "3c0a27b6addb4c6a977824995548fe90",
            "d14f2c65ef9348348250af2df0f32963",
            "eb23edb22d624aee9fd55ab05e2a517e",
            "a073d659b0f347c9b30c343430d7f6aa",
            "7508fbad995648a39d2a55953d000786",
            "9009dc5a656c4befbf513f2574df5a7d",
            "479d2f35498b4fbea77057f3f26fd287",
            "16b5928585984a1b81b809717f125489",
            "9d436c7ee90349a5966f025d095dd0cc",
            "3e503eeff5d94f0e9dc8a3d31190c6a2",
            "4abc5a8b257240b99535a6abe5e00ef6",
            "6c25264289e3411db29f6f8db936b00f",
            "a0ff8cd1b00544caa12cbd809524dd15",
            "1faac530b70b41a58a8c9c0cf44af69c",
            "7cfefcf49a9a426da4e9203ec4debe38",
            "5f96241543b441fcb13c16d4ca476405",
            "e6bbb6ce052045ca895b526c620d8847",
            "5c1c61cd14fb4aa8b335684fbf652a29",
            "a5a2bc5dc0b54a4796e574308bb6c4bc",
            "febe66177c604395a78be993eeff7c6d",
            "6de3445a866442aa9cc7d855cb0d0740",
            "353cb60a2a73417b81ccbb5e52480ed3",
            "69f323a406004822bf1a0bfd79767c9a",
            "88a8f5924a1e4a86805e314dd90c17be",
            "9ada44d21f234712ac5df797c730495a",
            "c777ac64366c40ea8a1f63408879c2ab"
          ]
        },
        "id": "93E9RdnR4D8v",
        "outputId": "6592f2ca-4d83-41a4-c650-564042a44b0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Batch 0, Loss: 0.14102648198604584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/100 [03:44<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@markdown ### **Training**\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[18], line 560\u001b[0m, in \u001b[0;36mRoboIMM.train\u001b[0;34m(self, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m    554\u001b[0m stop_gradient_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mys_r\u001b[39m\u001b[38;5;124m'\u001b[39m: pred_nograd\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_horizon, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_horizon)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    558\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(model_outputs, time_weights, stop_gradient_outputs)\n\u001b[0;32m--> 560\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    562\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/diffusion1/.venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/diffusion1/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/diffusion1/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@markdown ### **Training**\n",
        "imm.train(dataloader, None, num_epochs=100, lr=1e-4, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3hUbIuxGdO",
        "outputId": "36fa4685-4c4a-4ef3-a0a5-add134288f88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "\n",
        "ckpt_path = \"ckpts/model_checkpoint_100_100_0.0.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "imm.nets.load_state_dict(state_dict['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "b0a9f64f47d34769a47e165291d2c550",
            "2c31ffecd0494e2a8d0a8cefbea5df8a",
            "1d0a57ed8e914d31bcc1c36a56451df7",
            "e1bb86eb510b47d9ac2311926455bf1f",
            "182afbf676cd4c0982b90890bbdbeeef",
            "5d4a78f691ec4555bf179e5ef5d828ac",
            "1b96a78d8b8542eb830e3c446a0dcf42",
            "4cc800669dfd45a784864dd4a3881daa",
            "95dc6c44f97e4e8882a3657bd2fd66fb",
            "333b5148e59e47f3998a5cf5df531baf",
            "185ce2207cec4ae1a6ffdecadae23894"
          ]
        },
        "id": "OyLjlNQk5nr9",
        "outputId": "4641c055-a534-48e2-cc26-0d1056a004ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval PushTImageEnv: 201it [00:00, 281.47it/s, reward=0.727]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score:  0.727339455074238\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<video controls  width=\"256\"  height=\"256\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "env = PushTImageEnv()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon number of observations\n",
        "        images = np.stack([x['image'] for x in obs_deque])\n",
        "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
        "\n",
        "        # normalize observation\n",
        "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
        "        # images are already normalized to [0,1]\n",
        "        nimages = images\n",
        "\n",
        "        # device transfer\n",
        "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
        "        # (2,3,96,96)\n",
        "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
        "        # (2,2)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # get image features\n",
        "            # image_features = ema_nets['vision_encoder'](nimages)\n",
        "            # # (2,512)\n",
        "\n",
        "            # concat with low-dim observations\n",
        "            # obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
        "\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            # obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            naction = imm.sample(shape=(1, pred_horizon, action_dim), image=nimages.unsqueeze(0), agent_pos=nagent_poses.unsqueeze(0), steps=1)\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0873a2dee0e44b0fb3e8445d94c27171": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd765bcd2a34db49a02839ba1c4f518",
            "placeholder": "​",
            "style": "IPY_MODEL_d9054c34284045eda3546a21bb5fafc3",
            "value": "Epoch:   2%"
          }
        },
        "09152dbe3c8543aa809aa592afdc53f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b6383ed3b64b7e8c7e66d0e7a468d9",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdde5afac8041bf9b75e0e80bd80630",
            "value": " 2/100 [03:14&lt;2:38:57, 97.32s/it, loss=0.0192]"
          }
        },
        "11d14b9bf0f74bf7bd1b58e6a26fd49e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ef3c026774405cb91faf18d7fd8afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb23edb22d624aee9fd55ab05e2a517e",
            "placeholder": "​",
            "style": "IPY_MODEL_a073d659b0f347c9b30c343430d7f6aa",
            "value": " 379/379 [01:37&lt;00:00,  4.40it/s, loss=0.0185]"
          }
        },
        "16b5928585984a1b81b809717f125489": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1faac530b70b41a58a8c9c0cf44af69c",
            "placeholder": "​",
            "style": "IPY_MODEL_7cfefcf49a9a426da4e9203ec4debe38",
            "value": " 379/379 [01:37&lt;00:00,  4.57it/s, loss=0.0103]"
          }
        },
        "182afbf676cd4c0982b90890bbdbeeef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185ce2207cec4ae1a6ffdecadae23894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e913a07b044b2d91ca7b5d2dbbe8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d14b9bf0f74bf7bd1b58e6a26fd49e",
            "placeholder": "​",
            "style": "IPY_MODEL_e9fde27bd58b499bb386b36314d9efca",
            "value": ""
          }
        },
        "1b96a78d8b8542eb830e3c446a0dcf42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cdde5afac8041bf9b75e0e80bd80630": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d0a57ed8e914d31bcc1c36a56451df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cc800669dfd45a784864dd4a3881daa",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95dc6c44f97e4e8882a3657bd2fd66fb",
            "value": 200
          }
        },
        "1de97479fc3e44caae336cf0b1082d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5407db80343c44c79350021cd98204f8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68e9f5994766480dac5dc77ce043a2e9",
            "value": 0
          }
        },
        "1faac530b70b41a58a8c9c0cf44af69c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c31ffecd0494e2a8d0a8cefbea5df8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d4a78f691ec4555bf179e5ef5d828ac",
            "placeholder": "​",
            "style": "IPY_MODEL_1b96a78d8b8542eb830e3c446a0dcf42",
            "value": "Eval PushTImageEnv: "
          }
        },
        "333b5148e59e47f3998a5cf5df531baf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353cb60a2a73417b81ccbb5e52480ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c0a27b6addb4c6a977824995548fe90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e503eeff5d94f0e9dc8a3d31190c6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "410476fa4ac14dd2b78377f4d779402f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a92fc95c6a3d496997da90c87e24ba01",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a98272475d940b1b98438c4d02dcc05",
            "value": 2
          }
        },
        "44c486fa8c4241f4a1a245f4e24da769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0a27b6addb4c6a977824995548fe90",
            "max": 379,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d14f2c65ef9348348250af2df0f32963",
            "value": 379
          }
        },
        "479d2f35498b4fbea77057f3f26fd287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c25264289e3411db29f6f8db936b00f",
            "max": 379,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0ff8cd1b00544caa12cbd809524dd15",
            "value": 379
          }
        },
        "47b6383ed3b64b7e8c7e66d0e7a468d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4abc5a8b257240b99535a6abe5e00ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cc800669dfd45a784864dd4a3881daa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5124950790874225b5cca7556f122f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5407db80343c44c79350021cd98204f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "54c35b52c4e24543872c8335c935c410": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1c61cd14fb4aa8b335684fbf652a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f323a406004822bf1a0bfd79767c9a",
            "max": 379,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88a8f5924a1e4a86805e314dd90c17be",
            "value": 11
          }
        },
        "5d4a78f691ec4555bf179e5ef5d828ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f96241543b441fcb13c16d4ca476405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6bbb6ce052045ca895b526c620d8847",
              "IPY_MODEL_5c1c61cd14fb4aa8b335684fbf652a29",
              "IPY_MODEL_a5a2bc5dc0b54a4796e574308bb6c4bc"
            ],
            "layout": "IPY_MODEL_febe66177c604395a78be993eeff7c6d"
          }
        },
        "68e9f5994766480dac5dc77ce043a2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69f323a406004822bf1a0bfd79767c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c25264289e3411db29f6f8db936b00f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de3445a866442aa9cc7d855cb0d0740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b4b657d68648d9be98c099d061cd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c0237e38cfd4143aa423dea26637965",
              "IPY_MODEL_44c486fa8c4241f4a1a245f4e24da769",
              "IPY_MODEL_12ef3c026774405cb91faf18d7fd8afa"
            ],
            "layout": "IPY_MODEL_99c0577d549c4dd4a61d9ec0cf6254d5"
          }
        },
        "7508fbad995648a39d2a55953d000786": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9009dc5a656c4befbf513f2574df5a7d",
              "IPY_MODEL_479d2f35498b4fbea77057f3f26fd287",
              "IPY_MODEL_16b5928585984a1b81b809717f125489"
            ],
            "layout": "IPY_MODEL_9d436c7ee90349a5966f025d095dd0cc"
          }
        },
        "7cfefcf49a9a426da4e9203ec4debe38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fd765bcd2a34db49a02839ba1c4f518": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a8f5924a1e4a86805e314dd90c17be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c0237e38cfd4143aa423dea26637965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5124950790874225b5cca7556f122f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_f609505b750747dcaff8a950131743e9",
            "value": "Batch: 100%"
          }
        },
        "9009dc5a656c4befbf513f2574df5a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e503eeff5d94f0e9dc8a3d31190c6a2",
            "placeholder": "​",
            "style": "IPY_MODEL_4abc5a8b257240b99535a6abe5e00ef6",
            "value": "Batch: 100%"
          }
        },
        "95dc6c44f97e4e8882a3657bd2fd66fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99c0577d549c4dd4a61d9ec0cf6254d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9a98272475d940b1b98438c4d02dcc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ada44d21f234712ac5df797c730495a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d436c7ee90349a5966f025d095dd0cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a073d659b0f347c9b30c343430d7f6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0ff8cd1b00544caa12cbd809524dd15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5a2bc5dc0b54a4796e574308bb6c4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ada44d21f234712ac5df797c730495a",
            "placeholder": "​",
            "style": "IPY_MODEL_c777ac64366c40ea8a1f63408879c2ab",
            "value": " 11/379 [00:03&lt;01:34,  3.91it/s, loss=0.0256]"
          }
        },
        "a8e97e26d65147efa2ddb176214e818a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19e913a07b044b2d91ca7b5d2dbbe8f7",
              "IPY_MODEL_1de97479fc3e44caae336cf0b1082d02",
              "IPY_MODEL_b8d4e152c232416f82f5fb30e2478ef8"
            ],
            "layout": "IPY_MODEL_d66b3f8971b24c81abaefe111e6383a6"
          }
        },
        "a92fc95c6a3d496997da90c87e24ba01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a9f64f47d34769a47e165291d2c550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c31ffecd0494e2a8d0a8cefbea5df8a",
              "IPY_MODEL_1d0a57ed8e914d31bcc1c36a56451df7",
              "IPY_MODEL_e1bb86eb510b47d9ac2311926455bf1f"
            ],
            "layout": "IPY_MODEL_182afbf676cd4c0982b90890bbdbeeef"
          }
        },
        "b34bf5a5d603446fb453bbed31c4469c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0873a2dee0e44b0fb3e8445d94c27171",
              "IPY_MODEL_410476fa4ac14dd2b78377f4d779402f",
              "IPY_MODEL_09152dbe3c8543aa809aa592afdc53f1"
            ],
            "layout": "IPY_MODEL_e2a91fa6d1514913892c0def2e2ecf78"
          }
        },
        "b8d4e152c232416f82f5fb30e2478ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d00c195ac5644bd0aba81dccf56e2df0",
            "placeholder": "​",
            "style": "IPY_MODEL_54c35b52c4e24543872c8335c935c410",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "c777ac64366c40ea8a1f63408879c2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d00c195ac5644bd0aba81dccf56e2df0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d14f2c65ef9348348250af2df0f32963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d66b3f8971b24c81abaefe111e6383a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9054c34284045eda3546a21bb5fafc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1bb86eb510b47d9ac2311926455bf1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_333b5148e59e47f3998a5cf5df531baf",
            "placeholder": "​",
            "style": "IPY_MODEL_185ce2207cec4ae1a6ffdecadae23894",
            "value": " 201/? [00:34&lt;00:00,  6.11it/s, reward=0.828]"
          }
        },
        "e2a91fa6d1514913892c0def2e2ecf78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6bbb6ce052045ca895b526c620d8847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de3445a866442aa9cc7d855cb0d0740",
            "placeholder": "​",
            "style": "IPY_MODEL_353cb60a2a73417b81ccbb5e52480ed3",
            "value": "Batch:   3%"
          }
        },
        "e9fde27bd58b499bb386b36314d9efca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb23edb22d624aee9fd55ab05e2a517e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f609505b750747dcaff8a950131743e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "febe66177c604395a78be993eeff7c6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
