{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwO2gAgiJS2",
        "outputId": "acec0463-c03f-4359-9680-6ebbc99e4267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting diffusers\n",
            "  Downloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image\n",
            "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zarr\n",
            "  Downloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 KB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numcodecs\n",
            "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pygame\n",
            "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pymunk\n",
            "  Downloading pymunk-6.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 KB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting shapely\n",
            "  Downloading shapely-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ea/imm/venv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sympy==1.13.1\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0\n",
            "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.27.0\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 KB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio!=2.35.0,>=2.33\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 KB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21 in /home/ea/imm/venv/lib/python3.10/site-packages (from scikit-image) (24.2)\n",
            "Collecting tifffile>=2022.8.12\n",
            "  Downloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lazy-loader>=0.4\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Collecting scipy>=1.11.4\n",
            "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fasteners\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting asciitree\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting cffi>=1.17.1\n",
            "  Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.2/446.2 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym_notices>=0.0.4\n",
            "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Collecting cloudpickle>=1.2.0\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.42.1\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp>=3.20\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 KB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing legacy 'setup.py install' for asciitree, since package 'wheel' is not installed.\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827729 sha256=a2053bcbc9923c62d0632feaeae21364ea4fdf46d78588586b403958597c9e1e\n",
            "  Stored in directory: /home/ea/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, gym_notices, asciitree, zipp, urllib3, tqdm, sympy, safetensors, regex, pyyaml, pygame, pycparser, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, lazy-loader, idna, fsspec, filelock, fasteners, cloudpickle, charset-normalizer, certifi, tifffile, shapely, scipy, requests, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numcodecs, jinja2, importlib-metadata, imageio, gym, cffi, zarr, scikit-video, scikit-image, pymunk, nvidia-cusolver-cu12, huggingface-hub, torch, diffusers, torchvision\n",
            "  Running setup.py install for asciitree ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed MarkupSafe-3.0.2 asciitree-0.3.3 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 cloudpickle-3.1.1 diffusers-0.33.1 fasteners-0.19 filelock-3.18.0 fsspec-2025.3.2 gym-0.26.2 gym_notices-0.0.8 huggingface-hub-0.30.2 idna-3.10 imageio-2.37.0 importlib-metadata-8.6.1 jinja2-3.1.6 lazy-loader-0.4 mpmath-1.3.0 networkx-3.4.2 numcodecs-0.13.1 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opencv-python-4.11.0.86 pillow-11.2.1 pycparser-2.22 pygame-2.6.1 pymunk-6.11.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-image-0.25.2 scikit-video-1.1.11 scipy-1.15.2 shapely-2.1.0 sympy-1.13.1 tifffile-2025.3.30 torch-2.6.0 torchvision-0.21.0 tqdm-4.67.1 triton-3.2.0 urllib3-2.4.0 zarr-2.18.3 zipp-3.21.0\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!pip3 install torch torchvision diffusers \\\n",
        "scikit-image scikit-video zarr numcodecs \\\n",
        "pygame pymunk gym shapely opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "49d5ffd9eb81491f95df70e66c0c94e4",
            "9a3d8fbcc3e548e1a56d25c10b321e6a",
            "6cf470f12c174e94b0b058cfba9b5fb2",
            "9b6de90b3dbc4877966b2d5233189866",
            "4b3aeac3e8e74b8f8dab0dc9ad0b7c94",
            "ac6375a33ca84241b50638edb6321112",
            "583a530a9c0442f2b762b281caa4836d",
            "c7d2c3f8b3714a0d911a132c74589ce1",
            "03bbeb04a4c44206b1671e69864e69c7",
            "b2331069d969430b87c47e6f11fb2a9e",
            "c8c282ba14c14da28ba421abc7119cf9"
          ]
        },
        "id": "VrX4VTl5pYNq",
        "outputId": "1a01b12b-0e61-4f38-bb81-f72591e267da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ea/imm/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "L5E-nR6ornyg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "#@markdown\n",
        "#@markdown **Goal**: push the gray T-block into the green area.\n",
        "#@markdown\n",
        "#@markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "\n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "\n",
        "    def reset(self):\n",
        "        seed = self._seed\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "\n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for i in range(n_steps):\n",
        "                # Step PD control.\n",
        "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Step physics.\n",
        "                self.space.step(dt)\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "        terminated = done\n",
        "        truncated = done\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "\n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "\n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2],\n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "\n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # Add collision handeling\n",
        "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
        "        self.collision_handeler.post_solve = self._handle_collision\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OknH8Qfqrtc9",
        "outputId": "7563449e-9549-4de2-e049-bde843e76fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obs:  array([180.837 , 164.9424, 292.    , 351.    ,   2.9196])\n",
            "Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\n",
            "Action:  array([285.2624, 269.6187])\n",
            "Action:   [target_agent_x, target_agent_y]\n"
          ]
        }
      ],
      "source": [
        "# from huggingface_hub.utils import IGNORE_GIT_FOLDER_PATTERNS\n",
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "env = PushTEnv()\n",
        "\n",
        "# 1. seed env for initial state.\n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "env.seed(1000)\n",
        "\n",
        "# 2. must reset before use\n",
        "obs, IGNORE_GIT_FOLDER_PATTERNS = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    print(\"Obs: \", repr(obs))\n",
        "    print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
        "    print(\"Action: \", repr(action))\n",
        "    print(\"Action:   [target_agent_x, target_agent_y]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "vHepJOFBucwg"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTStateDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data (obs, action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of obs and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `obs`: shape (obs_horizon, obs_dim)\n",
        "#@markdown  - key `action`: shape (pred_horizon, action_dim)\n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int,\n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "\n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx,\n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx,\n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0),\n",
        "        'std': np.std(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTStateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path,\n",
        "                 pred_horizon, obs_horizon, action_horizon):\n",
        "\n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "        # All demonstration episodes are concatinated in the first dimension N\n",
        "        train_data = {\n",
        "            # (N, action_dim)\n",
        "            'action': dataset_root['data']['action'][:],\n",
        "            # (N, obs_dim)\n",
        "            'obs': dataset_root['data']['state'][:]\n",
        "        }\n",
        "        # Marks one-past the last index for each episode\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "\n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            # add padding such that each timestep in the dataset are seen\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "            stats[key+'_normalized'] = get_data_stats(normalized_train_data[key])\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "\n",
        "    def __len__(self):\n",
        "        # all possible segments of the dataset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['obs'] = nsample['obs'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZiHF3lzvB6k",
        "outputId": "474315be-970f-4a47-ca7e-d27d92a1711f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch['obs'].shape: torch.Size([256, 2, 5])\n",
            "batch['action'].shape torch.Size([256, 16, 2])\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "dataset = PushTStateDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24208\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float32(0.40121755)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset size\n",
        "print(len(dataset))\n",
        "stats['action_normalized']['std'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "X-XRB_g3vsgf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "# this is the unet for the robotics problem, modify this to use inductive moment matching\n",
        "# IMM\n",
        "\n",
        "'''\n",
        "η_t = t/α_t.\n",
        "OT-FM schedule: α_t = 1-t, σ_t = t\n",
        "x_t = α_t·x + σ_t·ε # ε ~ N(0, I)\n",
        "cout(t) = −t·σ_d\n",
        "\n",
        "'''\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Union\n",
        "from tqdm import tqdm\n",
        "class IMMloss(nn.Module):\n",
        "    \"\"\"\n",
        "    IMM loss function using the Laplace kernel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_horizon, pred_horizon, num_particles):\n",
        "        super(IMMloss, self).__init__()\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.num_particles = num_particles\n",
        "        \n",
        "    def laplace_kernel(self, x, y, w_scale, eps=0.006, dim_normalize=True):\n",
        "        \"\"\"\n",
        "        Laplace kernel: exp(w_scale * max(||x-y||_2, eps)/D)\n",
        "        \n",
        "        Args:\n",
        "            x, y: input tensors\n",
        "            w_scale: scaling factor (time-dependent)\n",
        "            eps: small constant to avoid undefined gradients\n",
        "            dim_normalize: whether to normalize by dimensionality D\n",
        "        \"\"\"\n",
        "        D = x.shape[-1] if dim_normalize else 1.0\n",
        "        distance = torch.norm(x - y, p=2, dim=-1)\n",
        "        # Apply max to avoid zero gradients\n",
        "        distance = torch.clamp(distance, min=eps)\n",
        "        return torch.exp(-w_scale * distance / D)\n",
        "    \n",
        "    def forward(self, model_outputs, time_weights, stop_gradient_outputs):\n",
        "        \"\"\"\n",
        "        Compute the IMM loss for a batch of model outputs.\n",
        "        \n",
        "        Args:\n",
        "            model_outputs: Dictionary containing:\n",
        "                - ys_t: outputs from time t to s [B, self.pred_horizon, self.obs_horizon]\n",
        "                - ys_r: outputs from time r to s [B, self.pred_horizon, self.obs_horizon]\n",
        "                - w_scale: time-dependent scaling factors [B]\n",
        "            time_weights: w(s,t) weights [B/M]\n",
        "            stop_gradient_outputs: Optional dictionary with same structure as model_outputs\n",
        "                                   containing the detached outputs (θ-)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Extract batch size and reshape for group processing\n",
        "        batch_size = model_outputs['ys_t'].shape[0]\n",
        "        M = self.num_particles\n",
        "        num_groups = batch_size // M\n",
        "        \n",
        "        # Flatten pred_horizon and obs_horizon dimensions before reshaping\n",
        "        # Reshape tensors to [num_groups, M, D]\n",
        "        ys_t = model_outputs['ys_t'].reshape(batch_size, -1).reshape(num_groups, M, -1)\n",
        "        ys_r_stop = stop_gradient_outputs['ys_r'].reshape(batch_size, -1).reshape(num_groups, M, -1)\n",
        "        w_scale = model_outputs['w_scale'].reshape(num_groups, M)\n",
        "        \n",
        "\n",
        "        \n",
        "        # Reshape time weights to [num_groups] by extracting the first element of each group\n",
        "        time_weights = time_weights.reshape(num_groups, M)[:,0].reshape(-1)\n",
        "        \n",
        "        total_loss = 0.0\n",
        "        for i in range(num_groups):\n",
        "            group_loss = 0.0\n",
        "            \n",
        "            # Compute the kernel matrices\n",
        "            for j in range(M):\n",
        "                for k in range(M):\n",
        "                    # First term: k(f_s,t^θ(x_t^(i,j)), f_s,t^θ(x_t^(i,k)))\n",
        "                    term1 = self.laplace_kernel(ys_t[i, j], ys_t[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Second term: k(f_s,r^θ-(x_r^(i,j)), f_s,r^θ-(x_r^(i,k)))\n",
        "                    term2 = self.laplace_kernel(ys_r_stop[i, j], ys_r_stop[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Third term: -2k(f_s,t^θ(x_t^(i,j)), f_s,r^θ-(x_r^(i,k)))\n",
        "                    term3 = -2.0 * self.laplace_kernel(ys_t[i, j], ys_r_stop[i, k], w_scale[i, j])\n",
        "                    \n",
        "                    # Sum up the terms\n",
        "                    group_loss += term1 + term2 + term3\n",
        "            \n",
        "            # Apply time-dependent weighting\n",
        "            group_loss = group_loss * time_weights[i] / (M * M)\n",
        "            total_loss += group_loss\n",
        "        \n",
        "        # Average over the number of groups\n",
        "        return total_loss / num_groups\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        \n",
        "        # Second encoder for timestep s (for IMM)\n",
        "        diffusion_step_encoder_s = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "            \n",
        "        # Total conditioning dimensions: t embedding + s embedding + global conditioning\n",
        "        cond_dim = dsed * 2 + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.diffusion_step_encoder_s = diffusion_step_encoder_s\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float],\n",
        "            timestep_s: Union[torch.Tensor, float],\n",
        "            global_cond):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step t\n",
        "        timestep_s: (B,) or int, diffusion step s (for IMM)\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time embedding for t\n",
        "        # timesteps = timestep\n",
        "        # if not torch.is_tensor(timesteps):\n",
        "        #     # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "        #     timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        # elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "        #     timesteps = timesteps[None].to(sample.device)\n",
        "        # # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        # timesteps = timesteps.expand(sample.shape[0])\n",
        "        \n",
        "        # Get time embedding for t\n",
        "        t_emb = self.diffusion_step_encoder(timestep)\n",
        "        \n",
        "        # 2. time embedding for s\n",
        "        s_emb = self.diffusion_step_encoder_s(timestep_s)\n",
        "        \n",
        "        # Combine t and s embeddings\n",
        "        global_feature = torch.cat([t_emb, s_emb], dim=-1)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n",
        "\n",
        "class RoboIMM:\n",
        "    \"\"\"\n",
        "    Simplified class for IMM with the robotics UNet.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        sigma_data,\n",
        "        obs_horizon,\n",
        "        pred_horizon,\n",
        "        num_particles\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the IMM sampler.\n",
        "        \n",
        "        Args:\n",
        "            model: The ConditionalUnet1D model\n",
        "            sigma_data: Data standard deviation\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.sigma_data = sigma_data\n",
        "        self.obs_horizon = obs_horizon\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.num_particles = num_particles\n",
        "\n",
        "        self.loss = IMMloss(obs_horizon=obs_horizon, pred_horizon=pred_horizon, num_particles=num_particles)\n",
        "\n",
        "    def get_alpha_sigma(self, t):\n",
        "        \"\"\"Get alpha and sigma values for time t.\"\"\"\n",
        "        # Using the \"flow matching\" schedule\n",
        "        alpha_t = (1 - t)\n",
        "        sigma_t = t\n",
        "        return alpha_t, sigma_t\n",
        "    \n",
        "    # def euler_step(self, yt, pred, t, s):\n",
        "    #     \"\"\"Euler step for flow matching.\"\"\"\n",
        "    #     return yt - (t - s) * self.sigma_data * pred\n",
        "    \n",
        "    # def edm_step(self, yt, pred, t, s):\n",
        "    #     \"\"\"EDM step for sampling.\"\"\"\n",
        "    #     alpha_t, sigma_t = self.get_alpha_sigma(t)\n",
        "    #     alpha_s, sigma_s = self.get_alpha_sigma(s)\n",
        "         \n",
        "    #     c_skip = (alpha_t * alpha_s + sigma_t * sigma_s) / (alpha_t**2 + sigma_t**2)\n",
        "    #     c_out = -(alpha_s * sigma_t - alpha_t * sigma_s) * (alpha_t**2 + sigma_t**2).rsqrt() * self.sigma_data\n",
        "        \n",
        "    #     return c_skip * yt + c_out * pred\n",
        "    \n",
        "    def ddim(self, yt, y, s, t):\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t)\n",
        "        alpha_s, sigma_s = self.get_alpha_sigma(s)\n",
        "\n",
        "        alpha_s = alpha_s.reshape(-1,1,1)\n",
        "        sigma_s = sigma_s.reshape(-1,1,1)\n",
        "        alpha_t = alpha_t.reshape(-1,1,1)\n",
        "        sigma_t = sigma_t.reshape(-1,1,1)\n",
        "        \n",
        "        ys = (alpha_s -   alpha_t * sigma_s / sigma_t) * y + sigma_s / sigma_t * yt\n",
        "        return ys\n",
        "    \n",
        "    def sample(self, shape, steps=20, global_cond=None, sampling_method=\"ddim\"):\n",
        "        \"\"\"\n",
        "        Generate samples using IMM sampling.\n",
        "        \n",
        "        Args:\n",
        "            shape: Shape of the samples to generate\n",
        "            steps: Number of sampling steps\n",
        "            global_cond: Global conditioning\n",
        "            sampling_method: \"ddim\"\n",
        "            \n",
        "        Returns:\n",
        "            Generated samples\n",
        "        \"\"\"\n",
        "        device = next(self.model.parameters()).device\n",
        "        \n",
        "        # Initialize with noise\n",
        "        x = torch.randn(shape, device=device) * self.sigma_data\n",
        "        \n",
        "        # Define time steps (uniform steps from 1 to 0)\n",
        "        times = torch.linspace(0.994, 0.006, steps + 1, device=device)\n",
        "        \n",
        "        for i in range(steps):\n",
        "            t = times[i]\n",
        "            s = times[i + 1]\n",
        "            \n",
        "            # Create batched time tensors\n",
        "            t_batch = torch.full((shape[0],), t, device=device)\n",
        "            s_batch = torch.full((shape[0],), s, device=device)\n",
        "            \n",
        "            # Run model forward\n",
        "            with torch.no_grad():\n",
        "                pred = self.predict(x, t_batch, s_batch, global_cond)\n",
        "            \n",
        "            # Apply sampling function based on method\n",
        "            # if sampling_method == \"ddim\":\n",
        "            #     x = self.ddim(x, pred, s_batch.view(-1, 1, 1), t_batch.view(-1, 1, 1))\n",
        "            # else:\n",
        "            #     raise ValueError(f\"Unknown sampling method: {sampling_method}\")\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def calculate_weights(self, s_times, t_times):\n",
        "        \"\"\"\n",
        "        Calculate the time-dependent weighting function w(s,t)\n",
        "        \"\"\"\n",
        "        b = 5  # Hyperparameter from paper\n",
        "        a = 1    # Hyperparameter from paper (a ∈ {1, 2})\n",
        "\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t_times)\n",
        "        \n",
        "        # Calculate log-SNR values\n",
        "        log_snr_t = 2 * torch.log(alpha_t / sigma_t)\n",
        "        dlog_snr_t = 2 / (torch.square(t_times) - t_times)\n",
        "        \n",
        "        # Calculate coefficient based on equation 13\n",
        "        sigmoid_term = torch.sigmoid(b - log_snr_t)\n",
        "        \n",
        "        snr_term = (alpha_t ** a) / (alpha_t ** 2 + sigma_t ** 2)\n",
        "        \n",
        "        return 0.5 * sigmoid_term * -1.0 * dlog_snr_t * snr_term\n",
        "    \n",
        "    def predict(self, xt, t, s, obs_cond):\n",
        "        c = 1000.0\n",
        "        cskip = 1.0\n",
        "        cout = -(t-s) * self.sigma_data\n",
        "        c_timestep = c * t\n",
        "        c_timestep_s = c * s\n",
        "        alpha_t, sigma_t = self.get_alpha_sigma(t)\n",
        "        c_in = (torch.pow(alpha_t, 2) + torch.pow(sigma_t, 2)).rsqrt() / self.sigma_data\n",
        "        xs = self.model(xt*c_in.reshape(-1,1,1), c_timestep, c_timestep_s, obs_cond)\n",
        "        return cskip * xt + cout * xs\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=100, lr=1e-4, device='cuda'):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Standard ADAM optimizer\n",
        "        # Note that EMA parametesr are not optimized\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            params=self.model.parameters(),\n",
        "            lr=lr, weight_decay=0)\n",
        "\n",
        "        # Cosine LR schedule\n",
        "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "            # epoch loop\n",
        "            for epoch_idx in tglobal:\n",
        "                # batch loop\n",
        "                with tqdm(train_loader, desc='Batch', leave=False) as tepoch:\n",
        "                    for batch_idx, batch in enumerate(tepoch):\n",
        "                        nobs = batch['obs'].to(device)\n",
        "                        naction = batch['action'].to(device)\n",
        "                        B = nobs.shape[0]\n",
        "\n",
        "                        num_groups = B // self.num_particles\n",
        "                        s_times = torch.rand(num_groups, device=device)\n",
        "                        t_times = s_times + (1 - s_times) * torch.rand(num_groups, device=device)\n",
        "                        r_times = s_times + (t_times - s_times) * torch.rand(num_groups, device=device)\n",
        "\n",
        "                        # times need to be shape (B,), currently they are shape (num_groups,)\n",
        "                        s_times = s_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "                        t_times = t_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "                        r_times = r_times.reshape(-1,1).expand(num_groups, self.num_particles).reshape(-1)\n",
        "\n",
        "                        noise = torch.randn_like(naction) * self.sigma_data\n",
        "\n",
        "                        x_t = self.ddim(yt=noise, y=naction, s=t_times, t=torch.ones_like(t_times))\n",
        "                        x_r = self.ddim(yt=x_t, y=naction, s=r_times, t=t_times)\n",
        "                                                \n",
        "                        # observation as FiLM conditioning\n",
        "                        # (B, obs_horizon, obs_dim)\n",
        "                        obs_cond = nobs[:,:self.obs_horizon,:]\n",
        "                        # (B, obs_horizon * obs_dim)\n",
        "                        obs_cond = obs_cond.flatten(start_dim=1)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        # pred_grad = self.model(x_t, t_times, s_times, obs_cond)\n",
        "                        pred_grad = self.predict(x_t, t_times, s_times, obs_cond)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            # pred_nograd = self.model(x_r, r_times, s_times, obs_cond)\n",
        "                            pred_nograd = self.predict(x_r, r_times, s_times, obs_cond)\n",
        "    \n",
        "                        time_weights = self.calculate_weights(s_times, t_times)\n",
        "                        \n",
        "                        # Reshape predictions to match expected dimensions\n",
        "                        # Assuming pred_grad and pred_nograd are [B, sequence_length, action_dim]\n",
        "                        # Reshape to [B, self.pred_horizon, self.obs_horizon]\n",
        "                        model_outputs = {\n",
        "                            'ys_t': pred_grad.reshape(B, self.pred_horizon, self.obs_horizon),\n",
        "                            'w_scale': 1.0 / torch.abs((t_times - s_times) * self.sigma_data)\n",
        "                        }\n",
        "                        \n",
        "                        stop_gradient_outputs = {\n",
        "                            'ys_r': pred_nograd.reshape(B, self.pred_horizon, self.obs_horizon).detach()\n",
        "                        }\n",
        "                        \n",
        "                        loss = self.loss(model_outputs, time_weights, stop_gradient_outputs)\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        lr_scheduler.step()\n",
        "\n",
        "                        if batch_idx % 10 == 0:\n",
        "                            print(f\"Epoch {epoch_idx}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "                        if batch_idx % 100 == 0 and epoch_idx % 10 == 0:\n",
        "                            # save model checkpoint\n",
        "                            torch.save({\n",
        "                                'epoch': epoch_idx,\n",
        "                                'model_state_dict': self.model.state_dict(),\n",
        "                                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                                'loss': loss.item()\n",
        "                            }, f\"ckpts/model_checkpoint_{epoch_idx}_{batch_idx}_{loss.item()}.pth\")\n",
        "                                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "51fbbfb0-deaf-48bd-cf4b-dc7f8f4ff246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 6.954880e+07\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# observation and action dimensions corrsponding to\n",
        "# the output of PushTEnv\n",
        "obs_dim = 5\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "imm = RoboIMM(\n",
        "    model=noise_pred_net,\n",
        "    sigma_data=stats['action_normalized']['std'].mean(),\n",
        "    obs_horizon=obs_horizon,\n",
        "    pred_horizon=pred_horizon,\n",
        "    num_particles=4\n",
        ")\n",
        "\n",
        "# example inputs\n",
        "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
        "diffusion_iter = torch.zeros((1,))\n",
        "diffusion_iter_s = torch.zeros((1,))\n",
        "\n",
        "\n",
        "r = imm.sample(shape=(1, pred_horizon, action_dim), steps=10, global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# # the noise prediction network\n",
        "# # takes noisy action, diffusion iteration and observation as input\n",
        "# # predicts the noise added to action\n",
        "# noise = noise_pred_net(\n",
        "#     sample=noised_action,\n",
        "#     timestep=diffusion_iter,\n",
        "#     timestep_s=diffusion_iter_s,\n",
        "#     global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# # illustration of removing noise\n",
        "# # the actual noise removal is performed by NoiseScheduler\n",
        "# # and is dependent on the diffusion noise schedule\n",
        "# denoised_action = noised_action - noise\n",
        "\n",
        "# # for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "# num_diffusion_iters = 100\n",
        "# noise_scheduler = DDPMScheduler(\n",
        "#     num_train_timesteps=num_diffusion_iters,\n",
        "#     # the choise of beta schedule has big impact on performance\n",
        "#     # we found squared cosine works the best\n",
        "#     beta_schedule='squaredcos_cap_v2',\n",
        "#     # clip output to [-1,1] to improve stability\n",
        "#     clip_sample=True,\n",
        "#     # our network predicts noise (instead of denoised action)\n",
        "#     prediction_type='epsilon'\n",
        "# )\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = noise_pred_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c29b0972b22d4dc59844ea958abf7bd1",
            "df7b2e7c39944341be1a091331fd7cc6",
            "0d687f5624cb4871ad167ebcbcb4c148",
            "82a5a8a6be3a41209de270d5072838a2",
            "6d4dfa99470e40559bebd6689305f155",
            "0964ed28f2794bdf91e2e2756aae3bc4",
            "1939a1c3cc7a498fa5f05ac737b0af99",
            "116ecf0deb6e44faadb594f2f982c4c2",
            "2db415245cbb44c8ab5c208be328fc16",
            "ed0c7c76b76d40c9b25ca92abb00cf34",
            "3a5463e9f9864ca2b52fe9cd28f4a8b8",
            "5352c2178612408aa84a3732d98a62ea",
            "f451b98692b64b54a0e8a3643a9bf992",
            "36c1a61163804f9a825638c6c8d962ed",
            "c2d84f324ecd4789b9b3420591f6186d",
            "6d4ec5c2f9624d398f0d9fc27d84cadb",
            "019444645b164b92a8da32e94a443d7e",
            "83bd2cb0ca534108804cdc298d8b26c6",
            "a26e0dce0a86491db71a731f570f1a80",
            "fe61754736d04d539c3f941049b5922a",
            "657a261a272d466e8bfd532279a401a1",
            "5b6eaafac1574f7481e3bb2e20e598b3"
          ]
        },
        "id": "93E9RdnR4D8v",
        "outputId": "51c50846-28b7-408c-c1d9-cd34154b57f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (256) must match the size of tensor b (2) at non-singleton dimension 2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 556\u001b[0m, in \u001b[0;36mRoboIMM.train\u001b[0;34m(self, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m    554\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# pred_grad = self.model(x_t, t_times, s_times, obs_cond)\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m pred_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# pred_nograd = self.model(x_r, r_times, s_times, obs_cond)\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     pred_nograd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x_r, r_times, s_times, obs_cond)\n",
            "Cell \u001b[0;32mIn[6], line 505\u001b[0m, in \u001b[0;36mRoboIMM.predict\u001b[0;34m(self, xt, t, s, obs_cond)\u001b[0m\n\u001b[1;32m    503\u001b[0m c_in \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mpow(alpha_t, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(sigma_t, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mrsqrt() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_data\n\u001b[1;32m    504\u001b[0m xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(xt\u001b[38;5;241m*\u001b[39mc_in\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), c_timestep, c_timestep_s, obs_cond)\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cskip \u001b[38;5;241m*\u001b[39m xt \u001b[38;5;241m+\u001b[39m \u001b[43mcout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (2) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "imm.train(dataloader, None, num_epochs=100, lr=1e-4, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3hUbIuxGdO",
        "outputId": "1c77956b-0006-403d-a9af-ce41def182bc"
      },
      "outputs": [],
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "\n",
        "ckpt_path = \"ckpts/model_checkpoint_85_0_-1.341576094660013e-08.pth\"\n",
        "\n",
        "state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "noise_pred_net.load_state_dict(state_dict['model_state_dict'])\n",
        "imm.model = noise_pred_net "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "1c17844c76d84675aa1c6e1a973308c8",
            "aa9489756ac24ba0b523f04a3352a09f",
            "51ff5f151bc9475fa3a5c54f8b60a50e",
            "58656f1d31ac45d3aaef45aa22f69899",
            "b9f98d2d7c624a8eae13fa5b422739ce",
            "b72ff2e7020f447fb492c1e9f69a0174",
            "567e3614e80743e5ae1f8e3c75a65b45",
            "187c8fd96f79429bb0752103d5998401",
            "17d07e24261b4627b1160beeb59c5636",
            "269772eda6cc4449bbba9e1e684b442c",
            "7cad813857df45f3ad617c74c68a619e"
          ]
        },
        "id": "OyLjlNQk5nr9",
        "outputId": "a104d1d1-f25a-456d-cac9-520ff50f455f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval PushTStateEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Eval PushTStateEnv: 201it [00:01, 198.31it/s, reward=0]                         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score:  0.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<video controls  width=\"256\"  height=\"256\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "env = PushTEnv()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon (2) number of observations\n",
        "        obs_seq = np.stack(obs_deque)\n",
        "        # normalize observation\n",
        "        nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
        "        # device transfer\n",
        "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "\n",
        "            r = imm.sample(shape=(1, pred_horizon, action_dim), steps=10, global_cond=obs_cond)\n",
        "            # for k in noise_scheduler.timesteps:\n",
        "            #     # predict noise\n",
        "            #     noise_pred = ema_noise_pred_net(\n",
        "            #         sample=naction,\n",
        "            #         timestep=k,\n",
        "            #         global_cond=obs_cond\n",
        "            #     )\n",
        "\n",
        "            #     # inverse diffusion step (remove noise)\n",
        "            #     naction = noise_scheduler.step(\n",
        "            #         model_output=noise_pred,\n",
        "            #         timestep=k,\n",
        "            #         sample=naction\n",
        "            #     ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019444645b164b92a8da32e94a443d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03bbeb04a4c44206b1671e69864e69c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0964ed28f2794bdf91e2e2756aae3bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d687f5624cb4871ad167ebcbcb4c148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116ecf0deb6e44faadb594f2f982c4c2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2db415245cbb44c8ab5c208be328fc16",
            "value": 1
          }
        },
        "116ecf0deb6e44faadb594f2f982c4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d07e24261b4627b1160beeb59c5636": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "187c8fd96f79429bb0752103d5998401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1939a1c3cc7a498fa5f05ac737b0af99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c17844c76d84675aa1c6e1a973308c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa9489756ac24ba0b523f04a3352a09f",
              "IPY_MODEL_51ff5f151bc9475fa3a5c54f8b60a50e",
              "IPY_MODEL_58656f1d31ac45d3aaef45aa22f69899"
            ],
            "layout": "IPY_MODEL_b9f98d2d7c624a8eae13fa5b422739ce"
          }
        },
        "269772eda6cc4449bbba9e1e684b442c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db415245cbb44c8ab5c208be328fc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36c1a61163804f9a825638c6c8d962ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a26e0dce0a86491db71a731f570f1a80",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe61754736d04d539c3f941049b5922a",
            "value": 95
          }
        },
        "3a5463e9f9864ca2b52fe9cd28f4a8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49d5ffd9eb81491f95df70e66c0c94e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a3d8fbcc3e548e1a56d25c10b321e6a",
              "IPY_MODEL_6cf470f12c174e94b0b058cfba9b5fb2",
              "IPY_MODEL_9b6de90b3dbc4877966b2d5233189866"
            ],
            "layout": "IPY_MODEL_4b3aeac3e8e74b8f8dab0dc9ad0b7c94"
          }
        },
        "4b3aeac3e8e74b8f8dab0dc9ad0b7c94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ff5f151bc9475fa3a5c54f8b60a50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187c8fd96f79429bb0752103d5998401",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17d07e24261b4627b1160beeb59c5636",
            "value": 200
          }
        },
        "5352c2178612408aa84a3732d98a62ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f451b98692b64b54a0e8a3643a9bf992",
              "IPY_MODEL_36c1a61163804f9a825638c6c8d962ed",
              "IPY_MODEL_c2d84f324ecd4789b9b3420591f6186d"
            ],
            "layout": "IPY_MODEL_6d4ec5c2f9624d398f0d9fc27d84cadb"
          }
        },
        "567e3614e80743e5ae1f8e3c75a65b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583a530a9c0442f2b762b281caa4836d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58656f1d31ac45d3aaef45aa22f69899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269772eda6cc4449bbba9e1e684b442c",
            "placeholder": "​",
            "style": "IPY_MODEL_7cad813857df45f3ad617c74c68a619e",
            "value": " 201/? [00:37&lt;00:00,  6.07it/s, reward=0.875]"
          }
        },
        "5b6eaafac1574f7481e3bb2e20e598b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "657a261a272d466e8bfd532279a401a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf470f12c174e94b0b058cfba9b5fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d2c3f8b3714a0d911a132c74589ce1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03bbeb04a4c44206b1671e69864e69c7",
            "value": 0
          }
        },
        "6d4dfa99470e40559bebd6689305f155": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4ec5c2f9624d398f0d9fc27d84cadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7cad813857df45f3ad617c74c68a619e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82a5a8a6be3a41209de270d5072838a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0c7c76b76d40c9b25ca92abb00cf34",
            "placeholder": "​",
            "style": "IPY_MODEL_3a5463e9f9864ca2b52fe9cd28f4a8b8",
            "value": " 1/1 [00:32&lt;00:00, 32.94s/it, loss=0.0523]"
          }
        },
        "83bd2cb0ca534108804cdc298d8b26c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a3d8fbcc3e548e1a56d25c10b321e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac6375a33ca84241b50638edb6321112",
            "placeholder": "​",
            "style": "IPY_MODEL_583a530a9c0442f2b762b281caa4836d",
            "value": ""
          }
        },
        "9b6de90b3dbc4877966b2d5233189866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2331069d969430b87c47e6f11fb2a9e",
            "placeholder": "​",
            "style": "IPY_MODEL_c8c282ba14c14da28ba421abc7119cf9",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "a26e0dce0a86491db71a731f570f1a80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9489756ac24ba0b523f04a3352a09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72ff2e7020f447fb492c1e9f69a0174",
            "placeholder": "​",
            "style": "IPY_MODEL_567e3614e80743e5ae1f8e3c75a65b45",
            "value": "Eval PushTStateEnv: "
          }
        },
        "ac6375a33ca84241b50638edb6321112": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2331069d969430b87c47e6f11fb2a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72ff2e7020f447fb492c1e9f69a0174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f98d2d7c624a8eae13fa5b422739ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c29b0972b22d4dc59844ea958abf7bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df7b2e7c39944341be1a091331fd7cc6",
              "IPY_MODEL_0d687f5624cb4871ad167ebcbcb4c148",
              "IPY_MODEL_82a5a8a6be3a41209de270d5072838a2"
            ],
            "layout": "IPY_MODEL_6d4dfa99470e40559bebd6689305f155"
          }
        },
        "c2d84f324ecd4789b9b3420591f6186d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_657a261a272d466e8bfd532279a401a1",
            "placeholder": "​",
            "style": "IPY_MODEL_5b6eaafac1574f7481e3bb2e20e598b3",
            "value": " 95/95 [00:32&lt;00:00,  3.08it/s, loss=0.0484]"
          }
        },
        "c7d2c3f8b3714a0d911a132c74589ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c8c282ba14c14da28ba421abc7119cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7b2e7c39944341be1a091331fd7cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0964ed28f2794bdf91e2e2756aae3bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_1939a1c3cc7a498fa5f05ac737b0af99",
            "value": "Epoch: 100%"
          }
        },
        "ed0c7c76b76d40c9b25ca92abb00cf34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f451b98692b64b54a0e8a3643a9bf992": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019444645b164b92a8da32e94a443d7e",
            "placeholder": "​",
            "style": "IPY_MODEL_83bd2cb0ca534108804cdc298d8b26c6",
            "value": "Batch: 100%"
          }
        },
        "fe61754736d04d539c3f941049b5922a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
